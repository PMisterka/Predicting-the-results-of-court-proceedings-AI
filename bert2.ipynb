{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zA3JmtCIxbY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import tensorflow as tf\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from tqdm import trange\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gSuZLCHFUEa7"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    cleaned_text = ' '.join([word for word in simple_preprocess(text) if word not in string.punctuation])\n",
        "    return cleaned_text\n",
        "\n",
        "def create_file_data(category_path, label):\n",
        "    file_data = defaultdict(str)\n",
        "\n",
        "    for filename in os.listdir(category_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(category_path, filename), 'r', encoding='utf-8-sig') as file:\n",
        "                text = file.read()\n",
        "                tokenized_text = clean(text)\n",
        "                file_data[filename] = {'text': tokenized_text, 'label': label}\n",
        "\n",
        "    return file_data\n",
        "\n",
        "file_data_k = create_file_data('/Users/julia/PycharmProjects/judgeAI-main/orzeczenia/kradziez', 'kradziez')\n",
        "file_data_o = create_file_data('/Users/julia/PycharmProjects/judgeAI-main/orzeczenia/oszustwo', 'oszustwo')\n",
        "file_data_z = create_file_data('/Users/julia/PycharmProjects/judgeAI-main/orzeczenia/zdrowie', 'przestępstwo przeciwko zdrowiu')\n",
        "file_data_ko = create_file_data('/Users/julia/PycharmProjects/judgeAI-main/orzeczenia/komunikacja', 'przestępstwo w komunikacji')\n",
        "\n",
        "merged_dict = {**file_data_k, **file_data_o, **file_data_z, **file_data_ko}\n",
        "\n",
        "csv_filename = 'data.csv'\n",
        "\n",
        "with open(csv_filename, 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    writer.writerow(['Key', 'Text', 'Label', 'Label_encoded'])\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    for key, values in merged_dict.items():\n",
        "        lem_text = values['text'].lower()\n",
        "        label_encoded = label_encoder.fit_transform([values['label']])[0]\n",
        "        writer.writerow([key, lem_text, values['label'], label_encoded])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "X = df.Text.values\n",
        "y = df.Label_encoded.values"
      ],
      "metadata": {
        "id": "cKd3nReznBCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GICd1WqcSwDx"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dkleczek/bert-base-polish-cased-v1', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qABnr8dRwDne"
      },
      "outputs": [],
      "source": [
        "def calculate_chunksize(text, tokenizer, target_length=512):\n",
        "\n",
        "    tokens = tokenizer.tokenize(text[1])\n",
        "    num_tokens = len(tokens)\n",
        "    return int((num_tokens + target_length) / target_length) * target_length\n",
        "\n",
        "chunksize = calculate_chunksize(X, tokenizer)\n",
        "chunks = []\n",
        "\n",
        "for idx in range(len(X)):\n",
        "    text = X[idx]\n",
        "    label = y[idx]\n",
        "\n",
        "    for i in range(0, len(text), chunksize):\n",
        "        chunk = {'Chunk': text[i:i+chunksize], 'Label': label}\n",
        "        chunks.append(chunk)\n",
        "\n",
        "df = pd.DataFrame(chunks)\n",
        "df.to_csv('new_data.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "df = pd.read_csv('new_data.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAehRwBMwDqC"
      },
      "outputs": [],
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "    return tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=512,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "for sample in df['Chunk']:\n",
        "    encoding_dict = preprocessing(sample, tokenizer)\n",
        "    token_id.append(encoding_dict['input_ids'])\n",
        "    attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "token_id = torch.cat(token_id, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "y = torch.tensor(df['Label'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NCj96aLxwDtb"
      },
      "outputs": [],
      "source": [
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(y)),\n",
        "    test_size=0.3,\n",
        "    shuffle=True,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], y[train_idx])\n",
        "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], y[val_idx])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_set,\n",
        "    sampler=RandomSampler(train_set),\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_set,\n",
        "    sampler=SequentialSampler(val_set),\n",
        "    batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuA6ZQkTRjaW"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'dkleczek/bert-base-polish-cased-v1',\n",
        "    num_labels = 4,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUI0NoOYRzbP"
      },
      "outputs": [],
      "source": [
        "model.cuda()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "total_confusion_matrix = np.zeros((4, 4), dtype=float)\n",
        "\n",
        "total_val_accuracy = []\n",
        "total_val_precision = []\n",
        "total_val_recall = []\n",
        "total_val_f1 = []\n",
        "\n",
        "for epoch in trange(epochs, desc='Epoch'):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_output = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_labels = []\n",
        "    val_predictions_probs = []\n",
        "\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "\n",
        "    epoch_confusion_matrix = np.zeros((4, 4), dtype=float)\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            eval_output = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask)\n",
        "\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        val_labels.extend(label_ids)\n",
        "        val_predictions_probs.extend(logits)\n",
        "\n",
        "        for target_label in range(4):\n",
        "            preds = np.argmax(logits, axis = 1).flatten()\n",
        "            labels = label_ids.flatten()\n",
        "\n",
        "            tp = sum([pred == labels[i] == target_label for i, pred in enumerate(preds)])\n",
        "            fp = sum([pred == target_label and pred != labels[i] for i, pred in enumerate(preds)])\n",
        "            tn = sum([pred != target_label and labels[i] != target_label for i, pred in enumerate(preds)])\n",
        "            fn = sum([pred != target_label and labels[i] == target_label for i, pred in enumerate(preds)])\n",
        "\n",
        "            epoch_confusion_matrix[target_label, target_label] += tp\n",
        "            for other_label in range(4):\n",
        "                if other_label != target_label:\n",
        "                    epoch_confusion_matrix[other_label, target_label] += fp\n",
        "                    epoch_confusion_matrix[target_label, other_label] += fn\n",
        "                    epoch_confusion_matrix[other_label, other_label] += tn\n",
        "\n",
        "            b_accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "            b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "            b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "\n",
        "            val_accuracy.append(b_accuracy)\n",
        "            if b_precision != 'nan':\n",
        "                val_precision.append(b_precision)\n",
        "            if b_recall != 'nan':\n",
        "                val_recall.append(b_recall)\n",
        "\n",
        "    epoch_val_accuracy = sum(val_accuracy) / len(val_accuracy)\n",
        "    epoch_val_precision = sum(val_precision) / len(val_precision) if len(val_precision) > 0 else 'nan'\n",
        "    epoch_val_recall = sum(val_recall) / len(val_recall) if len(val_recall) > 0 else 'nan'\n",
        "    epoch_val_f1 = (2 * epoch_val_precision * epoch_val_recall) / (epoch_val_precision + epoch_val_recall)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(epoch_val_accuracy))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(epoch_val_precision))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(epoch_val_recall))\n",
        "    print('\\t - Validation F1-score: {:.4f}'.format(epoch_val_f1))\n",
        "\n",
        "    total_confusion_matrix = total_confusion_matrix.astype(float) + epoch_confusion_matrix\n",
        "    total_val_accuracy.append(epoch_val_accuracy)\n",
        "    total_val_precision.append(epoch_val_precision)\n",
        "    total_val_recall.append(epoch_val_recall)\n",
        "    total_val_f1.append(epoch_val_f1)\n",
        "\n",
        "total_avg_val_accuracy = sum(total_val_accuracy) / len(total_val_accuracy)\n",
        "total_avg_val_precision = sum(total_val_precision) / len(total_val_precision) if len(total_val_precision) > 0 else 'nan'\n",
        "total_avg_val_recall = sum(total_val_recall) / len(total_val_recall) if len(total_val_recall) > 0 else 'nan'\n",
        "total_avg_val_f1 = sum(total_val_f1) / len(total_val_f1) if len(total_val_f1) > 0 else 'nan'\n",
        "\n",
        "normalized_conf_matrix = total_confusion_matrix.astype('float') / total_confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
        "plt.figure(figsize=(8, 6))\n",
        "labels = [\"kradzież\", \"oszustwo\", \"przestępstwo przeciwko zdrowiu\", \"przestępstwo w komunikacji\"]\n",
        "sns.heatmap(np.round(normalized_conf_matrix, 2) * 100, annot=True, fmt=\".0f\", cmap=\"Blues\", cbar_kws={'label': 'Procenty'}, xticklabels=labels, yticklabels=labels)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Przewidziana Etykieta')\n",
        "plt.ylabel('Rzeczywista Etykieta')\n",
        "plt.show()\n",
        "\n",
        "print(f'Total Validation Accuracy: {total_avg_val_accuracy:.4f}')\n",
        "print(f'Total Validation Precision: {total_avg_val_precision:.4f}')\n",
        "print(f'Total Validation Recall: {total_avg_val_recall:.4f}')\n",
        "print(f'Total Validation F1-score: {total_avg_val_f1:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}